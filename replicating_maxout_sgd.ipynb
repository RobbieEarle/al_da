{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replicating_maxout_sgd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobbieEarle/al_da/blob/master/replicating_maxout_sgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_zFVxTMVHKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import json\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "from pytz import timezone  \n",
        "\n",
        "import math\n",
        "import time\n",
        "import pprint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9rL8AfCW20K",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "##@title PyTorch NN Implementation - MaxOut\n",
        "\n",
        "class MaxNet(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               hidden_u1=32,\n",
        "               hidden_u2=16,\n",
        "               k=2,\n",
        "               l0_dropout_prob=0.5,\n",
        "               l1_dropout_prob=0.5,\n",
        "               l2_dropout_prob=0.5\n",
        "               ):\n",
        "    super(MaxNet, self).__init__()\n",
        "\n",
        "    self.k = k\n",
        "    self.fc1 = nn.Linear(784, hidden_u1)\n",
        "    self.fc2 = nn.Linear(int(math.ceil(hidden_u1 / k)), hidden_u2)\n",
        "    self.fc3 = nn.Linear(int(math.ceil(hidden_u2 / k)), 10)\n",
        "    self.l0_dropout = nn.Dropout(p=l0_dropout_prob)\n",
        "    self.l1_dropout = nn.Dropout(p=l1_dropout_prob)\n",
        "    self.l2_dropout = nn.Dropout(p=l2_dropout_prob)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.l0_dropout(x.view(-1, 28*28))\n",
        "    x = self.l1_dropout(self.maxout(self.fc1(x)))\n",
        "    x = self.l2_dropout(self.maxout(self.fc2(x)))\n",
        "    x = self.fc3(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  def maxout(self, x):\n",
        "    batch_size = x.size()[0]\n",
        "    num_hidden_nodes = x.size()[1]\n",
        "    num_groups = math.floor(num_hidden_nodes / self.k)\n",
        "    remainder = num_hidden_nodes % self.k\n",
        "\n",
        "    if remainder != 0:\n",
        "      y = x[:, num_hidden_nodes-remainder:]\n",
        "      x = x[:, :num_hidden_nodes-remainder]\n",
        "      y = torch.max(y, dim=1).values\n",
        "      y = y.view(y.size()[0],1)\n",
        "\n",
        "    x = x.view(batch_size,num_groups,self.k)\n",
        "    x = torch.max(x, dim=2).values\n",
        "\n",
        "    if remainder != 0:\n",
        "      x = torch.cat((x, y), dim=1)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6ePGozpL_Rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializes weights within given range\n",
        "irange = 0.005\n",
        "def weights_init(m):\n",
        "  if type(m) == nn.Linear:\n",
        "    m.weight.data.uniform_(-1 * irange, irange)\n",
        "    m.bias.data.fill_(0)\n",
        "\n",
        "# Applies max norm regularization\n",
        "def max_norm(model, max_val=1, eps=1e-8):\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'bias' not in name:\n",
        "      norm = param.norm(2, dim=1, keepdim=True)\n",
        "      desired = torch.clamp(norm, 0, max_val)\n",
        "      param = param * (desired / (eps + norm))\n",
        "\n",
        "def train_model(model, \n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                goal_loss=0,\n",
        "                curr_best_acc=0,\n",
        "                save_path=None,\n",
        "                curr_iteration=0,\n",
        "                curr_round=0,\n",
        "                init_lr=0.1,\n",
        "                init_momentum=0.5):\n",
        "\n",
        "  # ---- Initialization\n",
        "  if curr_round == 1:\n",
        "    model.apply(weights_init)\n",
        "    max_norm(model, max_val=1.9365)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=init_lr, momentum=init_momentum)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  scheduler = StepLR(optimizer, step_size=1, gamma=1/1.000004)\n",
        "  epoch = 1\n",
        "  curr_best_misclasses = 0\n",
        "  train_loss = 0\n",
        "  logs = {\n",
        "      \"iteration\":curr_iteration,\n",
        "      \"round\":curr_round,\n",
        "      \"curr_best_acc\":float(curr_best_acc),\n",
        "      \"data\":[]\n",
        "      }\n",
        "\n",
        "  while ((curr_round == 1 and curr_best_misclasses > 100) or (curr_round == 2 and train_loss > goal_loss) or epoch == 1) and epoch <= 250:\n",
        "\n",
        "    # ---- Training\n",
        "    model.train()\n",
        "    for batch_idx, (x, target) in enumerate(train_loader):\n",
        "      if torch.cuda.is_available():\n",
        "        x, target = x.cuda(), target.cuda()\n",
        "      optimizer.zero_grad()\n",
        "      out = model(x)\n",
        "      train_loss = criterion(out, target)\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      max_norm(model, max_val=1.9365)\n",
        "    for i in range(len(optimizer.param_groups)):\n",
        "      optimizer.param_groups[i]['momentum'] = min(0.7, optimizer.param_groups[i]['momentum'] + 0.0008)\n",
        "\n",
        "    # ---- Testing\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "    misclasses = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch_idx2, (x, target) in enumerate(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "          x, target = x.cuda(), target.cuda()\n",
        "        out = model(x)\n",
        "        test_loss = criterion(out, target)\n",
        "        _, prediction = torch.max(out.data, 1)\n",
        "        num_correct += torch.sum(prediction == target.data)\n",
        "        num_total += len(prediction)\n",
        "      misclasses += num_total - num_correct\n",
        "      accuracy = num_correct * 1.0 / num_total\n",
        "\n",
        "      if accuracy > curr_best_acc:\n",
        "        curr_best_acc = accuracy\n",
        "        path = save_path + \"best_acc_weights.pt\"\n",
        "        torch.save(model.state_dict(), path)\n",
        "\n",
        "    if misclasses < curr_best_misclasses or epoch == 1:\n",
        "      curr_best_misclasses = int(misclasses)\n",
        "\n",
        "    # ---- Logging\n",
        "    print (\n",
        "        \"  Epoch {} : lr = {:1.6f}  |  momentum = {:1.6f}  |  num_misclass = {}  |  loss = {:1.6f}  |  Accuracy = {:1.5f}\"\n",
        "        .format(epoch, optimizer.param_groups[0]['lr'], optimizer.param_groups[0]['momentum'], misclasses, train_loss, accuracy)\n",
        "    )\n",
        "    log = {\n",
        "        \"epoch\":epoch,\n",
        "        \"lr\":optimizer.param_groups[0]['lr'],\n",
        "        \"momentum\":optimizer.param_groups[0]['momentum'],\n",
        "        \"cross_entropy\":float(train_loss),\n",
        "        \"accuracy\":float(accuracy),\n",
        "        \"num_misclasses\":float(misclasses)\n",
        "    }\n",
        "    logs[\"data\"].append(log)\n",
        "\n",
        "    if epoch % 50 == 0 or curr_best_misclasses <= 100:\n",
        "      log_file_path = save_path + \"log.json\"\n",
        "      with open(log_file_path) as f:\n",
        "        log_file = json.load(f)\n",
        "      log_file[\"entries\"].append(logs)\n",
        "      with open(log_file_path, 'w') as f:\n",
        "          json.dump(log_file, f)\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "\n",
        "  return train_loss, curr_best_acc, model, optimizer.param_groups[0]['lr'], optimizer.param_groups[0]['momentum']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaDE_q-7EQVx",
        "colab_type": "code",
        "outputId": "f9a13a35-84a6-4336-ab3a-4c36c16f7de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "mnist_train_p2 = datasets.MNIST(root='./data', train=True, download=True, transform=trans)\n",
        "mnist_test_p2 = datasets.MNIST(root='./data', train=False, download=True, transform=trans)\n",
        "\n",
        "train_set_indices = np.arange(0,50000)\n",
        "mnist_train_p1 = torch.utils.data.Subset(mnist_train_p2, train_set_indices)\n",
        "test_set_indices = np.arange(50000, 60000)\n",
        "mnist_test_p1 = torch.utils.data.Subset(mnist_train_p2, test_set_indices)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader_p1 = torch.utils.data.DataLoader(dataset=mnist_train_p1, batch_size=batch_size, shuffle=True)\n",
        "test_loader_p1 = torch.utils.data.DataLoader(dataset=mnist_test_p1, batch_size=batch_size, shuffle=True)\n",
        "train_loader_p2 = torch.utils.data.DataLoader(dataset=mnist_train_p2, batch_size=batch_size, shuffle=True)\n",
        "test_loader_p2 = torch.utils.data.DataLoader(dataset=mnist_test_p2, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "curr_best_acc = 0\n",
        "iteration = 1\n",
        "drive.mount('/content/drive')\n",
        "path = F\"/content/drive/My Drive/combinact_outputs/maxout_sgd_training/training_iterations/\"\n",
        "log_file_path = path + \"log.json\"\n",
        "if not os.path.exists(log_file_path):\n",
        "  log_file = {\"entries\":[]}\n",
        "  with open(log_file_path, 'w') as f:\n",
        "      json.dump(log_file, f)\n",
        "\n",
        "while curr_best_acc < 0.99:\n",
        "  \n",
        "  model = MaxNet(240, 240, 5, 0, 0.2, 0)\n",
        "  if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "  print(\"------> Iteration \" + str(iteration) + \"  |  Round 1\")\n",
        "  cross_entropy, best_acc, model, round1_lr, round1_momentum = train_model(model,\n",
        "                                                                           train_loader=train_loader_p1, \n",
        "                                                                           test_loader=test_loader_p1, \n",
        "                                                                           curr_best_acc = curr_best_acc, \n",
        "                                                                           save_path=path, \n",
        "                                                                           curr_iteration=iteration,\n",
        "                                                                           curr_round=1)\n",
        "  \n",
        "  if best_acc > curr_best_acc:\n",
        "    curr_best_acc = best_acc\n",
        "\n",
        "  print(\"Best Accuracy: \" + str(curr_best_acc))\n",
        "  print(\"------> Iteration \" + str(iteration) + \"  |  Round 2\")\n",
        "  _, best_acc, _, _, _ = train_model(model, \n",
        "                                     train_loader=train_loader_p2, \n",
        "                                     test_loader=test_loader_p2, \n",
        "                                     goal_loss=cross_entropy,\n",
        "                                     curr_best_acc = curr_best_acc, \n",
        "                                     save_path=path, \n",
        "                                     curr_iteration=iteration, \n",
        "                                     curr_round = 2, \n",
        "                                     init_lr=round1_lr, \n",
        "                                     init_momentum=round1_momentum)\n",
        "  \n",
        "  if best_acc > curr_best_acc:\n",
        "    curr_best_acc = best_acc\n",
        "\n",
        "  print(\"Best Accuracy: \" + str(curr_best_acc))\n",
        "  iteration += 1\n",
        "\n",
        "drive.flush_and_unmount()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "------> Iteration 1  |  Round 1\n",
            "  Epoch 1 : lr = 0.099800  |  momentum = 0.500800  |  num_misclass = 7945  |  loss = 2.192814  |  Accuracy = 0.20550\n",
            "  Epoch 2 : lr = 0.099601  |  momentum = 0.501600  |  num_misclass = 1230  |  loss = 0.440342  |  Accuracy = 0.87700\n",
            "  Epoch 3 : lr = 0.099402  |  momentum = 0.502400  |  num_misclass = 587  |  loss = 0.187316  |  Accuracy = 0.94130\n",
            "  Epoch 4 : lr = 0.099203  |  momentum = 0.503200  |  num_misclass = 486  |  loss = 0.256267  |  Accuracy = 0.95140\n",
            "  Epoch 5 : lr = 0.099005  |  momentum = 0.504000  |  num_misclass = 420  |  loss = 0.319946  |  Accuracy = 0.95800\n",
            "  Epoch 6 : lr = 0.098807  |  momentum = 0.504800  |  num_misclass = 340  |  loss = 0.283217  |  Accuracy = 0.96600\n",
            "  Epoch 7 : lr = 0.098610  |  momentum = 0.505600  |  num_misclass = 409  |  loss = 0.340215  |  Accuracy = 0.95910\n",
            "  Epoch 8 : lr = 0.098413  |  momentum = 0.506400  |  num_misclass = 321  |  loss = 0.115779  |  Accuracy = 0.96790\n",
            "  Epoch 9 : lr = 0.098216  |  momentum = 0.507200  |  num_misclass = 283  |  loss = 0.061085  |  Accuracy = 0.97170\n",
            "  Epoch 10 : lr = 0.098020  |  momentum = 0.508000  |  num_misclass = 313  |  loss = 0.025733  |  Accuracy = 0.96870\n",
            "  Epoch 11 : lr = 0.097824  |  momentum = 0.508800  |  num_misclass = 258  |  loss = 0.034614  |  Accuracy = 0.97420\n",
            "  Epoch 12 : lr = 0.097629  |  momentum = 0.509600  |  num_misclass = 276  |  loss = 0.070752  |  Accuracy = 0.97240\n",
            "  Epoch 13 : lr = 0.097434  |  momentum = 0.510400  |  num_misclass = 250  |  loss = 0.099269  |  Accuracy = 0.97500\n",
            "  Epoch 14 : lr = 0.097239  |  momentum = 0.511200  |  num_misclass = 242  |  loss = 0.119766  |  Accuracy = 0.97580\n",
            "  Epoch 15 : lr = 0.097045  |  momentum = 0.512000  |  num_misclass = 283  |  loss = 0.060832  |  Accuracy = 0.97170\n",
            "  Epoch 16 : lr = 0.096851  |  momentum = 0.512800  |  num_misclass = 250  |  loss = 0.050417  |  Accuracy = 0.97500\n",
            "  Epoch 17 : lr = 0.096657  |  momentum = 0.513600  |  num_misclass = 225  |  loss = 0.022022  |  Accuracy = 0.97750\n",
            "  Epoch 18 : lr = 0.096464  |  momentum = 0.514400  |  num_misclass = 213  |  loss = 0.043275  |  Accuracy = 0.97870\n",
            "  Epoch 19 : lr = 0.096271  |  momentum = 0.515200  |  num_misclass = 217  |  loss = 0.028127  |  Accuracy = 0.97830\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFHOa7q3HK5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}